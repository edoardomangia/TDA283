I'm building a compiler in C++, for a language called Javalette. Here's a description of the language:

The Javalette language
======================

Javalette is a simple imperative language. It is almost a subset of C
(see below). It can also be easily translated to Java (see below).

Javalette is not a realistic language for production use. However, it is big
enough to allow for a core compiler project that illustrates all phases in
compilation. It also forms a basis for extensions in several directions.

The basic language has no heap-allocated data. However, the extensions involve
(Java-like) arrays, structures and objects, all of which are allocated on the
heap. The extended language is designed to be garbage-collected, but you will
not implement garbage collection as part of your project.

The description in this document is intentionally a bit vague and based on
examples; it is part of your task to define the language precisely.  However,
the language is also partly defined by a collection of test programs (see
below), on which the behaviour of your compiler is specified.

Example programs
-----------------------------

Let's start with a couple of small programs. First, here is how to say hello to
the world:

```java
// Hello world program

int main () {
  printString("Hello world!") ;
  return 0 ;
}
```

A program that prints the even numbers smaller than 10 is

```java
int main () {
  int i = 0 ;
  while (i < 10) {
    if (i % 2 == 0) printInt(i) ;
    i++ ;
  }
  return 0 ;
}
```

Finally, we show the factorial function in both iterative and recursive style:

```java
int main () {
  printInt(fact(7)) ;
  printInt(factr(7)) ;
  return 0 ;
}

// iterative factorial

int fact (int n) {
  int i,r ;
  i = 1 ;
  r = 1 ;
  while (i <= n) {
    r = r * i ;
    i++ ;
  }
  return r ;
}

// recursive factorial

int factr (int n) {
  if (n < 2)
    return 1 ;
  else
    return n * factr(n-1) ;
}
```

Program structure
-----------------

A Javalette program is a sequence of *function definitions*.

A function definition has a *return type*, a *name*, a *parameter list*, and a
*body* consisting of a *block*.

The names of the functions defined in a program must be different (i.e, there is
no overloading).

One function must have the name `main`. Its return type must be `int` and its
parameter list empty. Execution of a program consists of executing `main`.

A function whose return type is not `void` *must* return a value of its return
type. The compiler must check that it is not possible that execution of the
function terminates without passing a `return` statement. This check may be
conservative, i.e. reject as incorrect certain functions that actually would
always return a value.  A typical case could be to reject a function ending with
an `if`-statement where only one branch returns, without considering the
possibility that the test expression might always evaluate to the same value,
avoiding the branch without `return`. A function, whose return type is `void`, may, on the other hand, omit
the `return` statement completely.

Functions can be *mutually recursive*, i.e., call each other. There is no
prescribed order between function definitions (i.e., a call to a function may
appear in the program before the function definition).

There are no modules or other separate compilation facilities; we consider only
one-file programs.

Types
-----

Basic Javalette types are `int`, `double`, `boolean` and `void`.  Values of
types `int`, `double` and `boolean` are denoted by literals (see below). `void`
has no values and no literals.

No coercions (casts) are performed between types. Note this: it is NOT
considered an improvement to your compiler to add implicit casts. In fact, some
of the test programs check that you do not allow casts.

In the type checker, it is useful to have a notion of a *function type*, which
is a pair consisting of the value type and the list of parameter types.

Statements
----------

The following are the forms of statements in Javalette; we indicate syntax using
BNFC notation, where we use `Ident`, `Exp` and `Stmt` to indicate a variable,
expression and statement, respectively. Terminals are given within quotes. For
simplicity, we sometimes deviate here from the actual provided [grammar
file](/resources/Javalette.cf).

* *Empty statement*: `";"`
* *Variable declarations*: `Type Ident ";"`

    Comment: Several variables may be declared simultaneously, as in
    `int i, j;` and initial values may be specified, as in
    `int n = 0;`
* *Assignments*: `Ident "=" Exp ";"`
* *Increments and decrements*: `Ident "++" ";"` and `Ident "--" ";"`

    Comment: Only for variables of type `int`; can be seen as sugar for assignments.
* *Conditionals*:  `"if" "(" Exp ")" Stmt "else" Stmt`

    Comment: Can be without the `else` part.
* *While loops* : `"while" "(" Exp ")" Stmt`
* *Returns*: `"return" Exp ";"`

    Comment: No `Exp` for type `void`.
* *Expressions of type* `void`: `Exp ";"`

    Comment: The expression here will be a call to a void function (no other
    expressions have type `void`).
* *Blocks*: `"{" [Stmt] "}"`

    Comment: A function body is a statement of this form.

Declarations may appear anywhere within a block, but a variable must be declared
before it is used.

A variable declared in an outer scope may be redeclared in a block; the new
declaration then shadows the previous declaration for the rest of the block.

A variable can only be declared once in a block.

If no initial value is given in a variable declaration, the value of the
variable is initialized to `0` for type `int`, `0.0` for type `double` and
`false` for type `boolean`.  Note that this is different from Java, where local
variables must be explicitly initialized.

Expressions
-----------

Expressions in Javalette have the following forms:

* *Literals*: Integer, double, and Boolean literals (see below).
* *Variables*.
* *Binary operators*: `+`, `-`, `*`, `/` and
    `%`. Types are as expected; all except `%` are
    overloaded. Precedence and associativity as in C and Java.
* *Relational expressions*: `==`, `!=`,
    `<`, `<=`, `>` and `>=`. All overloaded
    as expected.
* *Disjunctions and conjunctions*: `||` and `&&`.
    These operators have *lazy semantics*, i.e.,

    * In `a && b`, if `a` evaluates to `false`,
        `b` is not evaluated and the value of the whole expression is `false`.
    * In `a || b`, if `a` evaluates to `true`,
        `b` is not evaluated and the value of the whole expression is `true`.
* *Unary operators*: `-` and `!`
    (negation of `int` and `double`, negation of `boolean`).
* Function calls.

Lexical details
---------------

Some of the tokens in Javalette are

* *Integer literals*: sequence of digits, e.g. `123`.
* *Float (double) literals*: digits with a decimal point, e.g. `3.14`,
possibly with an exponent (positive or negative), e.g. `1.6e-48`.
* *Boolean literals*:  `true` and `false`.
* *String literals*: ASCII characters in double quotes, e.g. `"Hello world"`
(escapes as usual: \verb#\n \t \" \\#). Can only be used in calls of
primitive function `printString`.
* *Identifiers*: a letter followed by an optional
sequence of letters, digits, and underscores.
* *Reserved words*: These include `while`,
  `if`, `else` and `return`.

Comments in Javalette are enclosed between `/*` and `*/` or extend from `//`
to the end of line, or from `#` to the end of line (to treat C preprocessor
directives as comments).

Primitive functions
-------------------

For input and output, Javalette programs may use the following functions:

```java
void printInt (int n)
void printDouble (double x)
void printString (String s)
int readInt ()
double readDouble ()
```

Note that there are no variables of type string in Javalette, so the only
argument that can be given to `printString` is a string literal.

The print functions print their arguments terminated by newline and the read
functions will only read one number per line. This is obviously rudimentary, but
enough for our purposes.

These functions are not directly implemented in the virtual machines we use. We
will provide them using other means, as detailed [below](#code_generation).

Parameter passing
-----------------

All parameters are passed by value, i.e., the value of the actual parameter is
computed and copied into the formal parameter before the subroutine is executed.
Parameters act as local variables within the subroutine, i.e., they can be
assigned to.

Javalette, C and Java
----------------------

Javalette programs can be compiled by a C compiler (`gcc`) if prefixed by
suitable preprocessor directives and macro definitions, e.g.

```c
#include <stdio.h>
#define printInt(k) printf("%d\n", k)
#define boolean int
#define true 1
```

In addition, function definitions must be reordered so that definition precedes
use, mutual recursion must be resolved by extra type signatures and variable
declarations moved to the beginnings of blocks.

Javalette programs can be compiled by a Java compiler (`javac`) by wrapping all
functions in a class as `public static` methods and adding one more `main`
method that calls your `main`:

```java
public static void main (String[] args) {
  main();
}
```

Using a C compiler or Java compiler is a good way to understand what a program
means even before you have written the full compiler. It can be useful to test
the programs produced by your compiler with the result of the C- and/or Java
compiler.




I uploaded Javalette.cf file, that describes the grammar of the language.
Now, the first step is to create a front-end for the compiler.
Here's a overview of what needs to be done:

The front end
=============

Your first task is to implement a compiler front end for Javalette:

1. Define suitable data types/classes for representing Javalette abstract syntax.
2. Implement a lexer and parser that builds abstract syntax from strings.
3. Implement a type checker that checks that programs are type-correct.
4. Implement a main program that calls lexer, parser and type checker, and reports errors.

These tasks are very well understood; there is a well-developed theory and, for
steps 1 and 2, convenient tools exist that do most of the work. You should be
familiar with these theories and tools and we expect you to complete the front
end during the first week of the course.

We recommend that you use the [BNF converter](https://bnfc.digitalgrammars.com/)
to build your lexer and parser. We also recommend you use `Alex` and `Happy` (if you
decide to implement your compiler in Haskell) or `JLex` and `Cup` (if you use Java).
We may also allow other implementation languages and tools, but we can not
guarantee support, and you must discuss your choice with the lecturer before you
start. This is to make sure that we will be able to run your compiler and that
you will not use inferior tools.

We provide a BNFC source file [Javalette.cf](/resources/Javalette.cf) that you may
use. If you already have a BNFC file for a similar language that you want to
reuse you may do so, but you must make sure that you modify it to pass the test
suite for this course.

We will accept a small number of shift/reduce conflicts in your parser; your
documentation must describe these and argue that they are harmless.
Reduce/reduce conflicts are not allowed. The provided BNFC file has the standard
dangling-else shift/reduce conflict.

One thing to note is that it may be useful to implement the type checker as a
function, which traverses the syntax *and returns its input* if the program is
type correct.  The reason for this is that you may actually want to modify this
and decorate the syntax trees with more information during type checking for
later use by the code generator. One example of such decoration can be to
annotate all subexpressions with type information; this will be useful during
code generation. To do this, you can add one further form of expression to your
BNFC source, namely a type-annotated expression.


 

For the front-end, I've used the BNFC program to generate the lexer (Lexer.C),
parser (Parser.C, Parser.H), abstract syntax (Absyn.C and Absyn.H). 
All the relevant files are uploaded.

I've also tried to implement a typechecker (TypeChecker.C and TypeChecker.H),
but it needs a review because I don't think it's correct.




For the back-end of the compiler, here's an overview on how one should proceed:

Code generation: LLVM
=====================

Once the [front end](frontend.md) is complete, your next task is to
implement code generation for LLVM, i.e. your task is to make your
compiler generate LLVM code for the given Javalette source code.

LLVM
----

LLVM (Low Level Virtual Machine) is both an intermediate
representation language and a compiler infrastructure, i.e. a
collection of software components for manipulating (e.g. optimizing)
LLVM code and backends for various architectures.  LLVM has a large
user base and is actively developed. A lot of information and code to
download can be found at the LLVM web site `http://www.llvm.org`. Make
sure your LLVM version is compatible with the one used in the testing
[Docker](/tester/Docker) has only guaranteed support for this
particular version.

Also LLVM code comes in two formats, a human-readable assembler format (stored
in `.ll` files) and a binary bitcode format (stored in`.bc` files). Your
compiler will produce the assembler format and you will use the LLVM assembler
`llvm-as` to produce binary files for execution.

In addition to the assembler, the LLVM infrastructure consists of a large number
of tools for optimizing, linking, JIT-compiling and manipulating bitcode. One
consequence is that a compiler writer may produce very simple-minded LLVM code
and leave to the LLVM tools to improve code when needed. Of course, similar
remarks apply to JVM code.


LLVM code
---------

The LLVM virtual machine is a *register machine*, with an infinite supply of
typed, virtual registers. The LLVM intermediate language is a version of
*three-address code* with arithmetic instructions that take operands from two
registers and place the result in a third register. LLVM code must be in SSA
(static single assignment) form, i.e. each virtual register may only be assigned
once in the program text.

The LLVM language is typed, and all instructions contain type information. This
"high-level" information, together with the "low-level" nature of the virtual
machine, gives LLVM a distinctive flavour.

The LLVM web site provides a wealth of information, including language
references, tutorials, tool manuals etc. There will also be lectures focusing on
code generation for LLVM.


The structure of a LLVM file
----------------------------

There is less overhead in the LLVM file. But, since the language is typed, we
must inform the tools of the types of the primitive functions:

```llvm
declare void @printInt(i32)
declare void @printDouble(double)
declare void @printString(i8*)
declare i32 @readInt()
declare double @readDouble()
```

Here `i32` is the type of 32 bit integers and `i8*` is the type of a pointer to
an 8 bit integer (i.e., to a character). Note that function names in LLVM always
start with `@`.

Before running a compiled Javalette program, `myfile.bc` must be linked with
`runtime.bc`, a file implementing the primitive functions, which we will
provide. In fact, this file is produced by giving `clang` a simple C file with
definitions such as

```c
void printInt(int x) {
  printf("%d\n",x);
}
```


An example
----------

The following LLVM code demonstrates some of the language features in LLVM. It
also serves as an example of what kind of code a Javalette compiler could
generate for the `fact` function described [here](javalette.md#example-programs).

```llvm
define i32 @main() {
entry:  %t0 = call i32 @fact(i32 7)             ; function call
        call void @printInt(i32 %t0)
        ret  i32 0

}

define i32 @fact(i32 %__p__n) {
entry:  %n = alloca i32                         ; allocate a variable on stack
        store i32 %__p__n , i32* %n             ; store parameter
        %i = alloca i32
        %r = alloca i32
        store i32 1 , i32* %i                   ; store initial values
        store i32 1 , i32* %r
        br label %lab0                          ; branch to lab0

lab0:   %t0 = load i32, i32* %i                 ; load i
        %t1 = load i32, i32* %n                 ; and n
        %t2 = icmp sle i32 %t0 , %t1            ; boolean %t2 will hold i <= n
        br i1 %t2 , label %lab1 , label %lab2   ; branch depending on %t2

lab1:   %t3 = load i32, i32* %r
        %t4 = load i32, i32* %i
        %t5 = mul i32 %t3 , %t4                 ; compute i * r
        store i32 %t5 , i32* %r                 ; store product
        %t6 = load i32, i32* %i                 ; fetch i,
        %t7 = add i32 %t6 , 1                   ; add 1
        store i32 %t7 , i32* %i                 ; and store
        br label %lab0

lab2:   %t8 = load i32, i32* %r
        ret  i32 %t8

}
```

We note several things:

* Registers and local variables have names starting with `%`.
* The syntax for function calls uses conventional parameter lists (with type
  info for each parameter).
* Booleans have type `i1`, one bit integers.
* After initialization, we branch explicitly to `lab0`, rather than just falling
  through.


LLVM tools
----------

Your compiler will generate a text file with LLVM code, which is conventionally
stored in files with suffix `.ll`. There are then several tools you might use:

* The *assembler* `llvm-as`, which translates the file to an equivalent binary
  format, called the *bitcode* format, stored in files with suffix `.bc` This is
  just a more efficient form for further processing. There is a *disassembler*
  `llvm-dis` that translates in the opposite direction.
* The *linker* `llvm-link`, which can be used to link together, e.g., `main.bc`
  with the bitcode file `runtime.bc` that defines the function `@printInt` and
  the other `IO` functions. By default, two files are written, `a.out` and
  `a.out.bc`.  As one can guess from the suffix, `a.out.bc` is a bitcode file
  which contains the definitions from all the input bitcode files.
* The *interpreter/JIT compiler* `lli`, which directly executes its bitcode file
  argument, using a Just-In-Time (JIT) compiler.
* The *static compiler* `llc`, which translates the file to a native assembler
  file for any of the supported architectures. It can also produce native object
  files using the flag `-filetype=obj`
* The *analyzer/optimizer* `opt`, which can perform a wide range of code
  optimizations of bitcode.
* The wrapper `clang` which uses various of the above tools together to provide
  a similar interface to `GCC`.

Note that some installations of LLVM require a version number after the tool
name, for example `llvm-as-3.8` instead of `llvm-as`.

Here are the steps you can use to produce an executable file from within your
compiler:

* Your compiler produces an LLVM file, let's call it `prog.ll`.
* Convert the file to bitcode format using `llvm-as`. For our example file,
  issue the command `llvm-as prog.ll`. This produces the file `prog.bc`.
* Link the bitcode file with the runtime file using `llvm-link`. This step
  requires that you give the name of the output file using the `-o` flag. For
  example we can name the output file `main.bc` like so: `llvm-link prog.bc
  runtime.bc -o main.bc`.
* Generate a native object file using `llc`. By default `llc` will produce
  assembler output, but by using the flag `-filetype=obj` it will produce an
  object file. The invocation will look like this: `llc -filetype=obj main.bc`
* Finally, produce an executable. The simplest way to do this is with
  `clang main.o`

A simpler alternative to the above steps is to let `clang` run the various
LLVM tools, with `clang prog.ll runtime.bc`

Also note that the [testing framework](/tester) will call LLVM itself, and
will link in the runtime library as well. For the purposes of assignment
submission, your compiler need only produce an LLVM file (the equivalent of
`prog.ll` above).

Optimizations
-------------

To whet your appetite, let us see how the LLVM code can be optimized:

```
> cat myfile.ll | llvm-as | opt -std-compile-opts | llvm-dis
```

```llvm
declare void @printInt(i32)

define i32 @main() {
entry:
	tail call void @printInt(i32 5040)
	ret i32 0
}

define i32 @fact(i32 %__p__n) nounwind readnone {
entry:
	%t23 = icmp slt i32 %__p__n, 1
	br i1 %t23, label %lab2, label %lab1

lab1:
	%indvar = phi i32 [ 0, %entry ], [ %i.01, %lab1 ]
	%r.02 = phi i32 [ 1, %entry ], [ %t5, %lab1 ]
	%i.01 = add i32 %indvar, 1
	%t5 = mul i32 %r.02, %i.01
	%t7 = add i32 %indvar, 2
	%t2 = icmp sgt i32 %t7, %__p__n
	br i1 %t2, label %lab2, label %lab1

lab2:
	%r.0.lcssa = phi i32 [ 1, %entry ], [ %t5, %lab1 ]
	ret i32 %r.0.lcssa
}
```

The first line above is the Unix command to do the optimization. We `cat` the
LLVM assembly code file and pipe it through the assembler, the optimizer and the
disassembler. The result is an optimized file, where we observe:

* In `main`, the call `fact(7)` has been completely computed to the result
  `5040`. The function `fact` is not necessary anymore, but remains, since we
  have not declared that `fact` is local to this file (one could do that).
* The definition of `fact` has been considerably optimized. In particular, there
  is no more any use of memory; the whole computation takes place in registers.
* We will explain the `phi` instruction in the lectures; the effect of the first
  instruction is that the value of `%indvar` will be 0 if control comes to
  `%lab1` from the block labelled `%entry` (i.e. the first time) and the value
  will be the value of `%i.01` if control comes from the block labelled `%lab1`
  (i.e. all other times). The `phi` instruction makes it possible to enforce the
  SSA form; there is only one assignment in the text to `%indvar`.

If we save the optimized code in `myfileOpt.bc` (without disassembling it), we
can link it together with the runtime using:

```
> llvm-link myfileOpt.bc runtime.bc -o a.out.bc
```

If we disassemble the resulting file `a.out.bc`, we get (we have edited the file
slightly in inessential ways):

```llvm
@fstr = internal constant [4 x i8] c"%d\0A\00"

define i32 @main() nounwind {
entry:
        %t0 = getelementptr [4 x i8]* @fstr, i32 0, i32 0
        %t1 = call i32 (i8*, ...)* @printf(i8* %t0, i32 5040) nounwind
        ret i32 0
}

declare i32 @printf(i8*, ...) nounwind
```

What remains is a definition of the format string `@fstr` as a global constant
(`\0A` is `\\n`), the `getelementpointer` instruction that returns a pointer to
the beginning of the format string and a call to `printf` with the result value.
Note that the call to `printInt` has been inlined, i.e., replaced by a call to
`printf`; so linking includes optimizations across files.

We can now run `a.out.bc` using the just-in-time compiler `lli`. Or, if we
prefer, we can produce native assembly code with `llc`. On a x86 machine, this
gives

```
        .text
        .align  4,0x90
        .globl  _main
_main:
        subl    $$12, %esp
        movl    $$5040, 4(%esp)
        movl    $$_fstr, (%esp)
        call    _printf
        xorl    %eax, %eax
        addl    $$12, %esp
        ret
        .cstring
_fstr:                          ## fstr
        .asciz  "%d\n"
```

I've tried to implement a code generator (CodeGen.C and CodeGen.H), but it's still not working.



I uploaded all the files I have at the moment, including a Makefile and 
a main function (main.C and main.H) to make everything I have until now run.


